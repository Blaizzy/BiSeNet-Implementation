# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OUZeQk_hfiCf4wjktr7VWW-4A4hTeQX1
"""

from google.colab import drive
from google.colab import files

#Accessing my google drive
drive.mount('/content/drive')

import keras
import tensorflow as tf
from keras.applications.xception import Xception,preprocess_input
from keras.optimizers import SGD
from keras.models import Model
from keras.layers import Conv2D, Input, Dense, Dropout, multiply, Dot, Concatenate,Add, GlobalAveragePooling2D
from keras.layers import BatchNormalization, Activation, AveragePooling2D, UpSampling2D
from keras.preprocessing.image import ImageDataGenerator
import matplotlib.pyplot as plt
from tensorflow.python.keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint
import os
import keras.backend as K
import time

from keras.layers.core import Lambda
from keras.backend import tf as ktf

#-----------------


def conv_bn_act(inputs, n_filters=64, kernel=(2, 2), strides=1, activation='relu'):

    conv = tf.keras.layers.Conv2D(n_filters, kernel_size= kernel, strides = strides, data_format='channels_last')(inputs)
    conv = tf.keras.layers.BatchNormalization()(conv)
    conv = tf.keras.layers.Activation(activation)(conv)

    return conv


def conv_act(inputs, n_filters, kernel = (1,1), activation = 'relu', pooling = False):
    if pooling:
        conv = tf.keras.layers.AveragePooling2D(pool_size=(1, 1), padding='same', data_format='channels_last')(inputs)
        conv = tf.keras.layers.Conv2D(n_filters, kernel_size= kernel, strides=1)(conv)
        conv = tf.keras.layers.Activation(activation)(conv)
    else:
        conv = tf.keras.layers.Conv2D(n_filters, kernel_size= kernel, strides=1)(inputs)
        conv = tf.keras.layers.Activation(activation)(conv)


    return conv


def CP_ARM(layer_13, layer_14):


    # Combine the up-sampled output feature of Global avg pooling and Xception features
    tail_avg = tf.keras.layers.GlobalAvgPool2D(data_format='channels_last')(layer_14)
    tail_upS = tf.keras.layers.UpSampling2D(size=(2, 2), data_format='channels_last')(layer_14)
    tail = tf.keras.layers.Add()([tail_avg, tail_upS])

    ARM_13 = ARM(layer_13, 1024)
    ARM_14 = ARM(layer_14, 2048)

    layer_13 = tf.keras.layers.UpSampling2D(size=2, data_format='channels_last')(ARM_13)
    layer_14 = tf.keras.layers.UpSampling2D(size=2, data_format='channels_last')(ARM_14)

    context_features = tf.keras.layers.Concatenate(axis=-1)([layer_14, layer_13])
    context_features = tf.keras.layers.Concatenate(axis=-1)([context_features, tail])

    context_features = tf.keras.layers.UpSampling2D(size=2, data_format='channels_last')(context_features)

    return context_features
# ARM (Attention Refinement Module)
# Refines features at each stage of the Context path

def ARM(inputs, n_filters):
    arm = tf.keras.layers.AveragePooling2D(pool_size=(1, 1), padding='same', data_format='channels_last')(inputs)
    arm = conv_bn_act(arm, n_filters, (1, 1), activation='sigmoid')
    arm = tf.keras.layers.multiply([inputs, arm])

    return arm


# FFM (Feature Fusion Module)
# Fuses the low level features of Spatial Path and high level
# features of Context Path

def FFM(input_sp, input_cp, n_classes):
    ffm = tf.keras.layers.Concatenate(axis=-1)([input_sp, input_cp])
    conv = conv_bn_act(ffm, n_classes, (3, 3), strides= 2)

    conv_1 = conv_act(conv, n_classes, (1,1), pooling= True)
    conv_1 = conv_act(conv_1, n_classes, (1,1), activation='sigmoid')

    ffm = tf.keras.layers.multiply([conv, conv_1])
    ffm = tf.keras.layers.Add()([conv, ffm])

    return ffm



#Model
inputs = tf.keras.layers.Input(shape=(224,224,3))


# Spatial Path
SP = conv_bn_act(inputs, 32, strides=2)
SP = conv_bn_act(SP, 64, strides=2)
SP = conv_bn_act(SP, 156, strides=2)


Xception_model = tf.keras.applications.Xception(weights='imagenet', input_shape = (224,224,3), include_top=False)

for layers in Xception_model.layers:
    layers.trainable = False

# Context_path
layer_13 = Xception_model.get_layer('block13_pool').output
layer_14 = Xception_model.output

CP_ARM = CP_ARM(layer_13, layer_14)

FFM = FFM(SP, CP_ARM, 3)


'''deconv_1 = tf.keras.layers.Conv2DTranspose(32, (3, 3), strides=(2, 2), padding='same')(FFM)
conv = tf.keras.layers.BatchNormalization()(deconv_1)
conv = tf.keras.layers.Activation('relu')(conv)

deconv_2 = tf.keras.layers.Conv2DTranspose(32, (3, 3), strides=(2, 2), padding='same')(conv)
conv = tf.keras.layers.BatchNormalization()(deconv_2)
conv = tf.keras.layers.Activation('relu')(conv)

deconv_2 = tf.keras.layers.Conv2DTranspose(32, (3, 3), strides=(2, 2), padding='same')(conv)
conv = tf.keras.layers.BatchNormalization()(deconv_2)
conv = tf.keras.layers.Activation('relu')(conv)



deconv_3 = tf.keras.layers.Conv2DTranspose(32, (3, 3), strides=(2, 2), padding='same')(conv)
conv = tf.keras.layers.BatchNormalization()(deconv_3)
output = tf.keras.layers.Activation('sigmoid')(conv)'''


output = tf.keras.layers.UpSampling2D(size=(16,16), data_format='channels_last')(FFM)

output = tf.keras.layers.ZeroPadding2D(padding=(8,8), data_format='channels_last')(output)



bisnet = tf.keras.models.Model(inputs = [inputs, Xception_model.input], outputs = output)

print(bisnet.summary())

#from keras.utils import plot_model
#plot_model(bisnet, to_file='model.png')

#Change dir to the file dir
os.chdir('/content/drive/My Drive/Colab Notebooks/python files/')



# dataset
import images
import numpy as np



train_set = images.load('/content/drive/My Drive/BiSeNet/dataset')

x, y, x_val, y_val = images.preprocess(train_set[0]), images.preprocess(train_set[1]), images.preprocess(train_set[2]), images.preprocess(train_set[3])

x = x[:367]
y_val = y_val[:101]


# Auxiliary loss function
n_classes = 32
def auxiliary_loss(y_true, y_pred):

    loss = tf.keras.backend.softmax(y_pred)
    loss = tf.keras.losses.categorical_crossentropy(y_true,loss)

    #loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=(y_pred, y_true), labels = n_classes)

    return loss

SGD = tf.keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=1e-4, nesterov=False)

print(len(x[:367]), len(y), len(x_val), len(y_val))

epochs = 5

bisnet.compile(loss= auxiliary_loss, optimizer=SGD, metrics=['accuracy'])

#Train and visualizing loss and acc in Tensorboard
tensorboard = tf.keras.callbacks.TensorBoard(log_dir='./Graph', histogram_freq=0,  
          write_graph=True, write_images=True,
                         write_grads=True)

callbacks = [EarlyStopping(monitor='loss', patience=2),
             ModelCheckpoint(filepath='best_model.h5', monitor='loss', save_best_only=True), tensorboard]

history = bisnet.fit([np.array(x), np.array(x)], np.array(y), epochs=epochs, batch_size=2, validation_data = ([np.array(x_val), np.array(x_val)], np.array(y_val)), callbacks = callbacks)


bisnet.save_weights('first_try.h5')


  # Plot training & validation accuracy values
plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

  # Plot training & validation loss values
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()



test_loss, test_acc = bisnet.evaluate([np.array(x_val),np.array(x_val)], np.array(y_val))

print('Test accuracy:', test_acc)

predictions = bisnet.predict([np.array(x_val),np.array(x_val)])

predictions = predictions[0]

print('Label predicted',np.argmax(predictions))


test = y_val[0]
print(test)

images.display_one(predictions)

